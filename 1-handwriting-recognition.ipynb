{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training data\n",
    "plt.figure(figsize=(5, 5))\n",
    "for k in range(12):\n",
    "    plt.subplot(3, 4, k + 1)\n",
    "    plt.imshow(X_train[k], cmap=\"Greys\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x172adfd50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test image\n",
    "plt.imshow(X_test[0], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X_train = X_train.reshape(60_000, 784).astype(\"float32\")\n",
    "X_test = X_test.reshape(10_000, 784).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting integer label to one-hot\n",
    "n_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing Model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"sigmoid\", input_shape=(784,)))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 64)                50240     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50890 (198.79 KB)\n",
      "Trainable params: 50890 (198.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\", optimizer=SGD(lr=0.1), metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "469/469 [==============================] - 1s 883us/step - loss: 2.1053 - accuracy: 0.4351 - val_loss: 1.8551 - val_accuracy: 0.6740\n",
      "Epoch 2/200\n",
      "469/469 [==============================] - 0s 775us/step - loss: 1.6541 - accuracy: 0.7082 - val_loss: 1.4494 - val_accuracy: 0.7549\n",
      "Epoch 3/200\n",
      "469/469 [==============================] - 0s 854us/step - loss: 1.3066 - accuracy: 0.7679 - val_loss: 1.1557 - val_accuracy: 0.8007\n",
      "Epoch 4/200\n",
      "469/469 [==============================] - 0s 820us/step - loss: 1.0658 - accuracy: 0.8048 - val_loss: 0.9592 - val_accuracy: 0.8272\n",
      "Epoch 5/200\n",
      "469/469 [==============================] - 0s 917us/step - loss: 0.9044 - accuracy: 0.8267 - val_loss: 0.8264 - val_accuracy: 0.8427\n",
      "Epoch 6/200\n",
      "469/469 [==============================] - 0s 817us/step - loss: 0.7931 - accuracy: 0.8411 - val_loss: 0.7330 - val_accuracy: 0.8549\n",
      "Epoch 7/200\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.7132 - accuracy: 0.8518 - val_loss: 0.6645 - val_accuracy: 0.8612\n",
      "Epoch 8/200\n",
      "469/469 [==============================] - 0s 867us/step - loss: 0.6535 - accuracy: 0.8589 - val_loss: 0.6122 - val_accuracy: 0.8680\n",
      "Epoch 9/200\n",
      "469/469 [==============================] - 0s 771us/step - loss: 0.6073 - accuracy: 0.8642 - val_loss: 0.5714 - val_accuracy: 0.8742\n",
      "Epoch 10/200\n",
      "469/469 [==============================] - 0s 760us/step - loss: 0.5706 - accuracy: 0.8691 - val_loss: 0.5384 - val_accuracy: 0.8788\n",
      "Epoch 11/200\n",
      "469/469 [==============================] - 0s 766us/step - loss: 0.5407 - accuracy: 0.8732 - val_loss: 0.5115 - val_accuracy: 0.8817\n",
      "Epoch 12/200\n",
      "469/469 [==============================] - 0s 762us/step - loss: 0.5160 - accuracy: 0.8762 - val_loss: 0.4888 - val_accuracy: 0.8856\n",
      "Epoch 13/200\n",
      "469/469 [==============================] - 0s 765us/step - loss: 0.4952 - accuracy: 0.8793 - val_loss: 0.4699 - val_accuracy: 0.8874\n",
      "Epoch 14/200\n",
      "469/469 [==============================] - 0s 814us/step - loss: 0.4773 - accuracy: 0.8820 - val_loss: 0.4535 - val_accuracy: 0.8895\n",
      "Epoch 15/200\n",
      "469/469 [==============================] - 0s 787us/step - loss: 0.4619 - accuracy: 0.8841 - val_loss: 0.4391 - val_accuracy: 0.8924\n",
      "Epoch 16/200\n",
      "469/469 [==============================] - 0s 780us/step - loss: 0.4484 - accuracy: 0.8860 - val_loss: 0.4267 - val_accuracy: 0.8934\n",
      "Epoch 17/200\n",
      "469/469 [==============================] - 0s 880us/step - loss: 0.4365 - accuracy: 0.8883 - val_loss: 0.4155 - val_accuracy: 0.8944\n",
      "Epoch 18/200\n",
      "469/469 [==============================] - 0s 849us/step - loss: 0.4258 - accuracy: 0.8897 - val_loss: 0.4058 - val_accuracy: 0.8963\n",
      "Epoch 19/200\n",
      "469/469 [==============================] - 0s 768us/step - loss: 0.4163 - accuracy: 0.8915 - val_loss: 0.3970 - val_accuracy: 0.8977\n",
      "Epoch 20/200\n",
      "469/469 [==============================] - 0s 822us/step - loss: 0.4077 - accuracy: 0.8931 - val_loss: 0.3890 - val_accuracy: 0.8986\n",
      "Epoch 21/200\n",
      "469/469 [==============================] - 0s 909us/step - loss: 0.3999 - accuracy: 0.8946 - val_loss: 0.3818 - val_accuracy: 0.9006\n",
      "Epoch 22/200\n",
      "469/469 [==============================] - 0s 735us/step - loss: 0.3927 - accuracy: 0.8959 - val_loss: 0.3751 - val_accuracy: 0.9015\n",
      "Epoch 23/200\n",
      "469/469 [==============================] - 0s 757us/step - loss: 0.3861 - accuracy: 0.8971 - val_loss: 0.3689 - val_accuracy: 0.9027\n",
      "Epoch 24/200\n",
      "469/469 [==============================] - 0s 790us/step - loss: 0.3800 - accuracy: 0.8978 - val_loss: 0.3633 - val_accuracy: 0.9036\n",
      "Epoch 25/200\n",
      "469/469 [==============================] - 0s 737us/step - loss: 0.3744 - accuracy: 0.8992 - val_loss: 0.3579 - val_accuracy: 0.9041\n",
      "Epoch 26/200\n",
      "469/469 [==============================] - 0s 778us/step - loss: 0.3692 - accuracy: 0.9003 - val_loss: 0.3533 - val_accuracy: 0.9047\n",
      "Epoch 27/200\n",
      "469/469 [==============================] - 0s 770us/step - loss: 0.3643 - accuracy: 0.9007 - val_loss: 0.3487 - val_accuracy: 0.9054\n",
      "Epoch 28/200\n",
      "469/469 [==============================] - 0s 772us/step - loss: 0.3596 - accuracy: 0.9018 - val_loss: 0.3443 - val_accuracy: 0.9061\n",
      "Epoch 29/200\n",
      "469/469 [==============================] - 0s 830us/step - loss: 0.3553 - accuracy: 0.9027 - val_loss: 0.3403 - val_accuracy: 0.9073\n",
      "Epoch 30/200\n",
      "469/469 [==============================] - 0s 785us/step - loss: 0.3512 - accuracy: 0.9034 - val_loss: 0.3365 - val_accuracy: 0.9079\n",
      "Epoch 31/200\n",
      "469/469 [==============================] - 0s 833us/step - loss: 0.3474 - accuracy: 0.9043 - val_loss: 0.3330 - val_accuracy: 0.9087\n",
      "Epoch 32/200\n",
      "469/469 [==============================] - 0s 756us/step - loss: 0.3437 - accuracy: 0.9049 - val_loss: 0.3296 - val_accuracy: 0.9087\n",
      "Epoch 33/200\n",
      "469/469 [==============================] - 0s 733us/step - loss: 0.3402 - accuracy: 0.9057 - val_loss: 0.3266 - val_accuracy: 0.9095\n",
      "Epoch 34/200\n",
      "469/469 [==============================] - 0s 756us/step - loss: 0.3369 - accuracy: 0.9066 - val_loss: 0.3235 - val_accuracy: 0.9104\n",
      "Epoch 35/200\n",
      "469/469 [==============================] - 0s 780us/step - loss: 0.3337 - accuracy: 0.9072 - val_loss: 0.3206 - val_accuracy: 0.9107\n",
      "Epoch 36/200\n",
      "469/469 [==============================] - 0s 733us/step - loss: 0.3307 - accuracy: 0.9079 - val_loss: 0.3180 - val_accuracy: 0.9116\n",
      "Epoch 37/200\n",
      "469/469 [==============================] - 0s 745us/step - loss: 0.3278 - accuracy: 0.9087 - val_loss: 0.3152 - val_accuracy: 0.9119\n",
      "Epoch 38/200\n",
      "469/469 [==============================] - 0s 741us/step - loss: 0.3250 - accuracy: 0.9094 - val_loss: 0.3126 - val_accuracy: 0.9124\n",
      "Epoch 39/200\n",
      "469/469 [==============================] - 0s 786us/step - loss: 0.3224 - accuracy: 0.9099 - val_loss: 0.3102 - val_accuracy: 0.9139\n",
      "Epoch 40/200\n",
      "469/469 [==============================] - 0s 742us/step - loss: 0.3198 - accuracy: 0.9106 - val_loss: 0.3078 - val_accuracy: 0.9136\n",
      "Epoch 41/200\n",
      "469/469 [==============================] - 0s 743us/step - loss: 0.3173 - accuracy: 0.9108 - val_loss: 0.3057 - val_accuracy: 0.9147\n",
      "Epoch 42/200\n",
      "469/469 [==============================] - 0s 732us/step - loss: 0.3149 - accuracy: 0.9115 - val_loss: 0.3035 - val_accuracy: 0.9151\n",
      "Epoch 43/200\n",
      "469/469 [==============================] - 0s 740us/step - loss: 0.3126 - accuracy: 0.9120 - val_loss: 0.3014 - val_accuracy: 0.9157\n",
      "Epoch 44/200\n",
      "469/469 [==============================] - 0s 736us/step - loss: 0.3104 - accuracy: 0.9127 - val_loss: 0.2995 - val_accuracy: 0.9160\n",
      "Epoch 45/200\n",
      "469/469 [==============================] - 0s 732us/step - loss: 0.3082 - accuracy: 0.9131 - val_loss: 0.2975 - val_accuracy: 0.9169\n",
      "Epoch 46/200\n",
      "469/469 [==============================] - 0s 731us/step - loss: 0.3061 - accuracy: 0.9138 - val_loss: 0.2955 - val_accuracy: 0.9170\n",
      "Epoch 47/200\n",
      "469/469 [==============================] - 0s 743us/step - loss: 0.3041 - accuracy: 0.9144 - val_loss: 0.2936 - val_accuracy: 0.9177\n",
      "Epoch 48/200\n",
      "469/469 [==============================] - 0s 734us/step - loss: 0.3021 - accuracy: 0.9147 - val_loss: 0.2918 - val_accuracy: 0.9183\n",
      "Epoch 49/200\n",
      "469/469 [==============================] - 0s 729us/step - loss: 0.3002 - accuracy: 0.9149 - val_loss: 0.2901 - val_accuracy: 0.9187\n",
      "Epoch 50/200\n",
      "469/469 [==============================] - 0s 740us/step - loss: 0.2983 - accuracy: 0.9157 - val_loss: 0.2885 - val_accuracy: 0.9185\n",
      "Epoch 51/200\n",
      "469/469 [==============================] - 0s 765us/step - loss: 0.2965 - accuracy: 0.9162 - val_loss: 0.2869 - val_accuracy: 0.9187\n",
      "Epoch 52/200\n",
      "469/469 [==============================] - 0s 804us/step - loss: 0.2947 - accuracy: 0.9166 - val_loss: 0.2855 - val_accuracy: 0.9199\n",
      "Epoch 53/200\n",
      "469/469 [==============================] - 0s 836us/step - loss: 0.2930 - accuracy: 0.9171 - val_loss: 0.2837 - val_accuracy: 0.9194\n",
      "Epoch 54/200\n",
      "469/469 [==============================] - 0s 763us/step - loss: 0.2913 - accuracy: 0.9176 - val_loss: 0.2822 - val_accuracy: 0.9201\n",
      "Epoch 55/200\n",
      "469/469 [==============================] - 0s 774us/step - loss: 0.2896 - accuracy: 0.9177 - val_loss: 0.2807 - val_accuracy: 0.9209\n",
      "Epoch 56/200\n",
      "469/469 [==============================] - 0s 797us/step - loss: 0.2880 - accuracy: 0.9180 - val_loss: 0.2794 - val_accuracy: 0.9209\n",
      "Epoch 57/200\n",
      "469/469 [==============================] - 0s 771us/step - loss: 0.2864 - accuracy: 0.9186 - val_loss: 0.2778 - val_accuracy: 0.9215\n",
      "Epoch 58/200\n",
      "469/469 [==============================] - 0s 761us/step - loss: 0.2849 - accuracy: 0.9191 - val_loss: 0.2765 - val_accuracy: 0.9212\n",
      "Epoch 59/200\n",
      "469/469 [==============================] - 0s 749us/step - loss: 0.2834 - accuracy: 0.9195 - val_loss: 0.2752 - val_accuracy: 0.9217\n",
      "Epoch 60/200\n",
      "469/469 [==============================] - 0s 775us/step - loss: 0.2819 - accuracy: 0.9200 - val_loss: 0.2739 - val_accuracy: 0.9223\n",
      "Epoch 61/200\n",
      "469/469 [==============================] - 0s 799us/step - loss: 0.2804 - accuracy: 0.9203 - val_loss: 0.2725 - val_accuracy: 0.9242\n",
      "Epoch 62/200\n",
      "469/469 [==============================] - 0s 778us/step - loss: 0.2790 - accuracy: 0.9207 - val_loss: 0.2712 - val_accuracy: 0.9231\n",
      "Epoch 63/200\n",
      "469/469 [==============================] - 0s 756us/step - loss: 0.2775 - accuracy: 0.9208 - val_loss: 0.2701 - val_accuracy: 0.9239\n",
      "Epoch 64/200\n",
      "469/469 [==============================] - 0s 854us/step - loss: 0.2762 - accuracy: 0.9214 - val_loss: 0.2688 - val_accuracy: 0.9242\n",
      "Epoch 65/200\n",
      "469/469 [==============================] - 0s 823us/step - loss: 0.2748 - accuracy: 0.9215 - val_loss: 0.2677 - val_accuracy: 0.9248\n",
      "Epoch 66/200\n",
      "469/469 [==============================] - 0s 824us/step - loss: 0.2735 - accuracy: 0.9223 - val_loss: 0.2664 - val_accuracy: 0.9251\n",
      "Epoch 67/200\n",
      "469/469 [==============================] - 0s 783us/step - loss: 0.2722 - accuracy: 0.9225 - val_loss: 0.2652 - val_accuracy: 0.9260\n",
      "Epoch 68/200\n",
      "469/469 [==============================] - 0s 802us/step - loss: 0.2709 - accuracy: 0.9228 - val_loss: 0.2642 - val_accuracy: 0.9258\n",
      "Epoch 69/200\n",
      "469/469 [==============================] - 0s 927us/step - loss: 0.2697 - accuracy: 0.9233 - val_loss: 0.2629 - val_accuracy: 0.9265\n",
      "Epoch 70/200\n",
      "469/469 [==============================] - 0s 763us/step - loss: 0.2684 - accuracy: 0.9236 - val_loss: 0.2618 - val_accuracy: 0.9265\n",
      "Epoch 71/200\n",
      "469/469 [==============================] - 0s 803us/step - loss: 0.2672 - accuracy: 0.9237 - val_loss: 0.2609 - val_accuracy: 0.9270\n",
      "Epoch 72/200\n",
      "469/469 [==============================] - 0s 776us/step - loss: 0.2660 - accuracy: 0.9243 - val_loss: 0.2597 - val_accuracy: 0.9273\n",
      "Epoch 73/200\n",
      "469/469 [==============================] - 0s 805us/step - loss: 0.2648 - accuracy: 0.9245 - val_loss: 0.2585 - val_accuracy: 0.9273\n",
      "Epoch 74/200\n",
      "469/469 [==============================] - 0s 757us/step - loss: 0.2636 - accuracy: 0.9247 - val_loss: 0.2576 - val_accuracy: 0.9280\n",
      "Epoch 75/200\n",
      "469/469 [==============================] - 0s 792us/step - loss: 0.2624 - accuracy: 0.9252 - val_loss: 0.2564 - val_accuracy: 0.9282\n",
      "Epoch 76/200\n",
      "469/469 [==============================] - 0s 822us/step - loss: 0.2613 - accuracy: 0.9254 - val_loss: 0.2555 - val_accuracy: 0.9287\n",
      "Epoch 77/200\n",
      "469/469 [==============================] - 0s 767us/step - loss: 0.2602 - accuracy: 0.9259 - val_loss: 0.2546 - val_accuracy: 0.9290\n",
      "Epoch 78/200\n",
      "469/469 [==============================] - 0s 777us/step - loss: 0.2591 - accuracy: 0.9259 - val_loss: 0.2535 - val_accuracy: 0.9288\n",
      "Epoch 79/200\n",
      "469/469 [==============================] - 0s 843us/step - loss: 0.2580 - accuracy: 0.9263 - val_loss: 0.2526 - val_accuracy: 0.9289\n",
      "Epoch 80/200\n",
      "469/469 [==============================] - 0s 770us/step - loss: 0.2569 - accuracy: 0.9266 - val_loss: 0.2517 - val_accuracy: 0.9291\n",
      "Epoch 81/200\n",
      "469/469 [==============================] - 0s 739us/step - loss: 0.2559 - accuracy: 0.9270 - val_loss: 0.2507 - val_accuracy: 0.9293\n",
      "Epoch 82/200\n",
      "469/469 [==============================] - 0s 745us/step - loss: 0.2548 - accuracy: 0.9273 - val_loss: 0.2498 - val_accuracy: 0.9298\n",
      "Epoch 83/200\n",
      "469/469 [==============================] - 0s 808us/step - loss: 0.2538 - accuracy: 0.9277 - val_loss: 0.2489 - val_accuracy: 0.9301\n",
      "Epoch 84/200\n",
      "469/469 [==============================] - 0s 849us/step - loss: 0.2527 - accuracy: 0.9279 - val_loss: 0.2480 - val_accuracy: 0.9301\n",
      "Epoch 85/200\n",
      "469/469 [==============================] - 0s 837us/step - loss: 0.2517 - accuracy: 0.9283 - val_loss: 0.2471 - val_accuracy: 0.9305\n",
      "Epoch 86/200\n",
      "469/469 [==============================] - 0s 843us/step - loss: 0.2507 - accuracy: 0.9283 - val_loss: 0.2462 - val_accuracy: 0.9306\n",
      "Epoch 87/200\n",
      "469/469 [==============================] - 0s 769us/step - loss: 0.2497 - accuracy: 0.9286 - val_loss: 0.2453 - val_accuracy: 0.9307\n",
      "Epoch 88/200\n",
      "469/469 [==============================] - 0s 760us/step - loss: 0.2488 - accuracy: 0.9290 - val_loss: 0.2444 - val_accuracy: 0.9308\n",
      "Epoch 89/200\n",
      "469/469 [==============================] - 0s 936us/step - loss: 0.2478 - accuracy: 0.9292 - val_loss: 0.2435 - val_accuracy: 0.9316\n",
      "Epoch 90/200\n",
      "469/469 [==============================] - 0s 773us/step - loss: 0.2468 - accuracy: 0.9292 - val_loss: 0.2427 - val_accuracy: 0.9316\n",
      "Epoch 91/200\n",
      "469/469 [==============================] - 0s 761us/step - loss: 0.2459 - accuracy: 0.9302 - val_loss: 0.2417 - val_accuracy: 0.9317\n",
      "Epoch 92/200\n",
      "469/469 [==============================] - 0s 845us/step - loss: 0.2450 - accuracy: 0.9298 - val_loss: 0.2410 - val_accuracy: 0.9319\n",
      "Epoch 93/200\n",
      "469/469 [==============================] - 0s 766us/step - loss: 0.2440 - accuracy: 0.9304 - val_loss: 0.2405 - val_accuracy: 0.9320\n",
      "Epoch 94/200\n",
      "469/469 [==============================] - 0s 792us/step - loss: 0.2431 - accuracy: 0.9307 - val_loss: 0.2395 - val_accuracy: 0.9317\n",
      "Epoch 95/200\n",
      "469/469 [==============================] - 0s 813us/step - loss: 0.2422 - accuracy: 0.9311 - val_loss: 0.2385 - val_accuracy: 0.9322\n",
      "Epoch 96/200\n",
      "469/469 [==============================] - 0s 787us/step - loss: 0.2413 - accuracy: 0.9313 - val_loss: 0.2378 - val_accuracy: 0.9321\n",
      "Epoch 97/200\n",
      "469/469 [==============================] - 0s 759us/step - loss: 0.2404 - accuracy: 0.9315 - val_loss: 0.2370 - val_accuracy: 0.9320\n",
      "Epoch 98/200\n",
      "469/469 [==============================] - 0s 813us/step - loss: 0.2395 - accuracy: 0.9315 - val_loss: 0.2362 - val_accuracy: 0.9321\n",
      "Epoch 99/200\n",
      "469/469 [==============================] - 0s 800us/step - loss: 0.2386 - accuracy: 0.9319 - val_loss: 0.2356 - val_accuracy: 0.9327\n",
      "Epoch 100/200\n",
      "469/469 [==============================] - 0s 805us/step - loss: 0.2378 - accuracy: 0.9321 - val_loss: 0.2348 - val_accuracy: 0.9325\n",
      "Epoch 101/200\n",
      "469/469 [==============================] - 0s 808us/step - loss: 0.2369 - accuracy: 0.9325 - val_loss: 0.2340 - val_accuracy: 0.9329\n",
      "Epoch 102/200\n",
      "469/469 [==============================] - 0s 809us/step - loss: 0.2361 - accuracy: 0.9326 - val_loss: 0.2333 - val_accuracy: 0.9328\n",
      "Epoch 103/200\n",
      "469/469 [==============================] - 0s 814us/step - loss: 0.2352 - accuracy: 0.9330 - val_loss: 0.2326 - val_accuracy: 0.9333\n",
      "Epoch 104/200\n",
      "469/469 [==============================] - 0s 764us/step - loss: 0.2344 - accuracy: 0.9334 - val_loss: 0.2317 - val_accuracy: 0.9334\n",
      "Epoch 105/200\n",
      "469/469 [==============================] - 0s 782us/step - loss: 0.2335 - accuracy: 0.9334 - val_loss: 0.2310 - val_accuracy: 0.9332\n",
      "Epoch 106/200\n",
      "469/469 [==============================] - 0s 786us/step - loss: 0.2328 - accuracy: 0.9337 - val_loss: 0.2302 - val_accuracy: 0.9339\n",
      "Epoch 107/200\n",
      "469/469 [==============================] - 0s 766us/step - loss: 0.2320 - accuracy: 0.9341 - val_loss: 0.2294 - val_accuracy: 0.9339\n",
      "Epoch 108/200\n",
      "469/469 [==============================] - 0s 738us/step - loss: 0.2311 - accuracy: 0.9343 - val_loss: 0.2287 - val_accuracy: 0.9340\n",
      "Epoch 109/200\n",
      "469/469 [==============================] - 0s 846us/step - loss: 0.2304 - accuracy: 0.9346 - val_loss: 0.2280 - val_accuracy: 0.9339\n",
      "Epoch 110/200\n",
      "469/469 [==============================] - 0s 833us/step - loss: 0.2296 - accuracy: 0.9349 - val_loss: 0.2273 - val_accuracy: 0.9347\n",
      "Epoch 111/200\n",
      "469/469 [==============================] - 0s 800us/step - loss: 0.2288 - accuracy: 0.9348 - val_loss: 0.2266 - val_accuracy: 0.9352\n",
      "Epoch 112/200\n",
      "469/469 [==============================] - 0s 806us/step - loss: 0.2280 - accuracy: 0.9352 - val_loss: 0.2260 - val_accuracy: 0.9344\n",
      "Epoch 113/200\n",
      "469/469 [==============================] - 0s 786us/step - loss: 0.2272 - accuracy: 0.9354 - val_loss: 0.2253 - val_accuracy: 0.9350\n",
      "Epoch 114/200\n",
      "469/469 [==============================] - 0s 792us/step - loss: 0.2265 - accuracy: 0.9355 - val_loss: 0.2246 - val_accuracy: 0.9352\n",
      "Epoch 115/200\n",
      "469/469 [==============================] - 0s 775us/step - loss: 0.2257 - accuracy: 0.9358 - val_loss: 0.2239 - val_accuracy: 0.9355\n",
      "Epoch 116/200\n",
      "469/469 [==============================] - 0s 799us/step - loss: 0.2250 - accuracy: 0.9359 - val_loss: 0.2233 - val_accuracy: 0.9353\n",
      "Epoch 117/200\n",
      "469/469 [==============================] - 0s 971us/step - loss: 0.2242 - accuracy: 0.9360 - val_loss: 0.2225 - val_accuracy: 0.9362\n",
      "Epoch 118/200\n",
      "469/469 [==============================] - 0s 851us/step - loss: 0.2235 - accuracy: 0.9363 - val_loss: 0.2217 - val_accuracy: 0.9366\n",
      "Epoch 119/200\n",
      "469/469 [==============================] - 0s 801us/step - loss: 0.2227 - accuracy: 0.9364 - val_loss: 0.2211 - val_accuracy: 0.9358\n",
      "Epoch 120/200\n",
      "469/469 [==============================] - 0s 780us/step - loss: 0.2220 - accuracy: 0.9369 - val_loss: 0.2205 - val_accuracy: 0.9360\n",
      "Epoch 121/200\n",
      "469/469 [==============================] - 0s 782us/step - loss: 0.2213 - accuracy: 0.9366 - val_loss: 0.2199 - val_accuracy: 0.9367\n",
      "Epoch 122/200\n",
      "469/469 [==============================] - 0s 817us/step - loss: 0.2206 - accuracy: 0.9371 - val_loss: 0.2193 - val_accuracy: 0.9368\n",
      "Epoch 123/200\n",
      "469/469 [==============================] - 0s 800us/step - loss: 0.2198 - accuracy: 0.9373 - val_loss: 0.2187 - val_accuracy: 0.9365\n",
      "Epoch 124/200\n",
      "469/469 [==============================] - 0s 785us/step - loss: 0.2191 - accuracy: 0.9374 - val_loss: 0.2179 - val_accuracy: 0.9369\n",
      "Epoch 125/200\n",
      "469/469 [==============================] - 0s 854us/step - loss: 0.2184 - accuracy: 0.9376 - val_loss: 0.2172 - val_accuracy: 0.9373\n",
      "Epoch 126/200\n",
      "469/469 [==============================] - 0s 763us/step - loss: 0.2177 - accuracy: 0.9380 - val_loss: 0.2167 - val_accuracy: 0.9369\n",
      "Epoch 127/200\n",
      "469/469 [==============================] - 0s 805us/step - loss: 0.2170 - accuracy: 0.9378 - val_loss: 0.2160 - val_accuracy: 0.9375\n",
      "Epoch 128/200\n",
      "469/469 [==============================] - 0s 837us/step - loss: 0.2164 - accuracy: 0.9381 - val_loss: 0.2155 - val_accuracy: 0.9375\n",
      "Epoch 129/200\n",
      "469/469 [==============================] - 0s 843us/step - loss: 0.2157 - accuracy: 0.9385 - val_loss: 0.2149 - val_accuracy: 0.9382\n",
      "Epoch 130/200\n",
      "469/469 [==============================] - 0s 857us/step - loss: 0.2150 - accuracy: 0.9386 - val_loss: 0.2143 - val_accuracy: 0.9384\n",
      "Epoch 131/200\n",
      "469/469 [==============================] - 0s 808us/step - loss: 0.2143 - accuracy: 0.9388 - val_loss: 0.2137 - val_accuracy: 0.9384\n",
      "Epoch 132/200\n",
      "469/469 [==============================] - 0s 801us/step - loss: 0.2136 - accuracy: 0.9392 - val_loss: 0.2130 - val_accuracy: 0.9382\n",
      "Epoch 133/200\n",
      "469/469 [==============================] - 0s 778us/step - loss: 0.2130 - accuracy: 0.9392 - val_loss: 0.2124 - val_accuracy: 0.9386\n",
      "Epoch 134/200\n",
      "469/469 [==============================] - 0s 823us/step - loss: 0.2123 - accuracy: 0.9396 - val_loss: 0.2120 - val_accuracy: 0.9385\n",
      "Epoch 135/200\n",
      "469/469 [==============================] - 0s 818us/step - loss: 0.2117 - accuracy: 0.9396 - val_loss: 0.2112 - val_accuracy: 0.9391\n",
      "Epoch 136/200\n",
      "469/469 [==============================] - 0s 775us/step - loss: 0.2110 - accuracy: 0.9399 - val_loss: 0.2107 - val_accuracy: 0.9391\n",
      "Epoch 137/200\n",
      "469/469 [==============================] - 0s 797us/step - loss: 0.2104 - accuracy: 0.9400 - val_loss: 0.2101 - val_accuracy: 0.9391\n",
      "Epoch 138/200\n",
      "469/469 [==============================] - 0s 871us/step - loss: 0.2098 - accuracy: 0.9403 - val_loss: 0.2095 - val_accuracy: 0.9391\n",
      "Epoch 139/200\n",
      "469/469 [==============================] - 0s 795us/step - loss: 0.2091 - accuracy: 0.9403 - val_loss: 0.2090 - val_accuracy: 0.9395\n",
      "Epoch 140/200\n",
      "469/469 [==============================] - 0s 802us/step - loss: 0.2085 - accuracy: 0.9405 - val_loss: 0.2084 - val_accuracy: 0.9398\n",
      "Epoch 141/200\n",
      "469/469 [==============================] - 0s 758us/step - loss: 0.2078 - accuracy: 0.9406 - val_loss: 0.2079 - val_accuracy: 0.9402\n",
      "Epoch 142/200\n",
      "469/469 [==============================] - 0s 784us/step - loss: 0.2072 - accuracy: 0.9409 - val_loss: 0.2072 - val_accuracy: 0.9403\n",
      "Epoch 143/200\n",
      "469/469 [==============================] - 0s 793us/step - loss: 0.2066 - accuracy: 0.9411 - val_loss: 0.2067 - val_accuracy: 0.9401\n",
      "Epoch 144/200\n",
      "469/469 [==============================] - 0s 775us/step - loss: 0.2060 - accuracy: 0.9414 - val_loss: 0.2061 - val_accuracy: 0.9404\n",
      "Epoch 145/200\n",
      "469/469 [==============================] - 0s 780us/step - loss: 0.2054 - accuracy: 0.9413 - val_loss: 0.2056 - val_accuracy: 0.9409\n",
      "Epoch 146/200\n",
      "469/469 [==============================] - 0s 762us/step - loss: 0.2048 - accuracy: 0.9418 - val_loss: 0.2051 - val_accuracy: 0.9409\n",
      "Epoch 147/200\n",
      "469/469 [==============================] - 0s 759us/step - loss: 0.2042 - accuracy: 0.9417 - val_loss: 0.2044 - val_accuracy: 0.9410\n",
      "Epoch 148/200\n",
      "469/469 [==============================] - 0s 831us/step - loss: 0.2036 - accuracy: 0.9418 - val_loss: 0.2041 - val_accuracy: 0.9407\n",
      "Epoch 149/200\n",
      "469/469 [==============================] - 0s 804us/step - loss: 0.2029 - accuracy: 0.9420 - val_loss: 0.2036 - val_accuracy: 0.9409\n",
      "Epoch 150/200\n",
      "469/469 [==============================] - 0s 777us/step - loss: 0.2024 - accuracy: 0.9424 - val_loss: 0.2030 - val_accuracy: 0.9414\n",
      "Epoch 151/200\n",
      "469/469 [==============================] - 0s 808us/step - loss: 0.2018 - accuracy: 0.9427 - val_loss: 0.2025 - val_accuracy: 0.9417\n",
      "Epoch 152/200\n",
      "469/469 [==============================] - 0s 790us/step - loss: 0.2012 - accuracy: 0.9427 - val_loss: 0.2019 - val_accuracy: 0.9418\n",
      "Epoch 153/200\n",
      "469/469 [==============================] - 0s 838us/step - loss: 0.2006 - accuracy: 0.9428 - val_loss: 0.2015 - val_accuracy: 0.9421\n",
      "Epoch 154/200\n",
      "469/469 [==============================] - 0s 802us/step - loss: 0.2001 - accuracy: 0.9432 - val_loss: 0.2008 - val_accuracy: 0.9422\n",
      "Epoch 155/200\n",
      "469/469 [==============================] - 0s 790us/step - loss: 0.1995 - accuracy: 0.9432 - val_loss: 0.2004 - val_accuracy: 0.9420\n",
      "Epoch 156/200\n",
      "469/469 [==============================] - 0s 803us/step - loss: 0.1989 - accuracy: 0.9437 - val_loss: 0.1999 - val_accuracy: 0.9424\n",
      "Epoch 157/200\n",
      "469/469 [==============================] - 0s 822us/step - loss: 0.1983 - accuracy: 0.9435 - val_loss: 0.1996 - val_accuracy: 0.9424\n",
      "Epoch 158/200\n",
      "469/469 [==============================] - 0s 825us/step - loss: 0.1978 - accuracy: 0.9438 - val_loss: 0.1988 - val_accuracy: 0.9429\n",
      "Epoch 159/200\n",
      "469/469 [==============================] - 0s 858us/step - loss: 0.1972 - accuracy: 0.9439 - val_loss: 0.1984 - val_accuracy: 0.9420\n",
      "Epoch 160/200\n",
      "469/469 [==============================] - 0s 841us/step - loss: 0.1967 - accuracy: 0.9442 - val_loss: 0.1978 - val_accuracy: 0.9426\n",
      "Epoch 161/200\n",
      "469/469 [==============================] - 0s 898us/step - loss: 0.1961 - accuracy: 0.9442 - val_loss: 0.1973 - val_accuracy: 0.9426\n",
      "Epoch 162/200\n",
      "469/469 [==============================] - 0s 883us/step - loss: 0.1956 - accuracy: 0.9447 - val_loss: 0.1967 - val_accuracy: 0.9433\n",
      "Epoch 163/200\n",
      "469/469 [==============================] - 0s 825us/step - loss: 0.1950 - accuracy: 0.9444 - val_loss: 0.1964 - val_accuracy: 0.9434\n",
      "Epoch 164/200\n",
      "469/469 [==============================] - 0s 801us/step - loss: 0.1945 - accuracy: 0.9447 - val_loss: 0.1959 - val_accuracy: 0.9433\n",
      "Epoch 165/200\n",
      "469/469 [==============================] - 0s 945us/step - loss: 0.1939 - accuracy: 0.9452 - val_loss: 0.1953 - val_accuracy: 0.9441\n",
      "Epoch 166/200\n",
      "469/469 [==============================] - 0s 863us/step - loss: 0.1934 - accuracy: 0.9452 - val_loss: 0.1948 - val_accuracy: 0.9445\n",
      "Epoch 167/200\n",
      "469/469 [==============================] - 0s 834us/step - loss: 0.1929 - accuracy: 0.9451 - val_loss: 0.1945 - val_accuracy: 0.9439\n",
      "Epoch 168/200\n",
      "469/469 [==============================] - 0s 839us/step - loss: 0.1923 - accuracy: 0.9456 - val_loss: 0.1941 - val_accuracy: 0.9442\n",
      "Epoch 169/200\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.9458 - val_loss: 0.1936 - val_accuracy: 0.9445\n",
      "Epoch 170/200\n",
      "469/469 [==============================] - 0s 844us/step - loss: 0.1913 - accuracy: 0.9459 - val_loss: 0.1930 - val_accuracy: 0.9450\n",
      "Epoch 171/200\n",
      "469/469 [==============================] - 0s 854us/step - loss: 0.1908 - accuracy: 0.9463 - val_loss: 0.1926 - val_accuracy: 0.9448\n",
      "Epoch 172/200\n",
      "469/469 [==============================] - 0s 956us/step - loss: 0.1903 - accuracy: 0.9463 - val_loss: 0.1921 - val_accuracy: 0.9449\n",
      "Epoch 173/200\n",
      "469/469 [==============================] - 0s 804us/step - loss: 0.1898 - accuracy: 0.9463 - val_loss: 0.1917 - val_accuracy: 0.9455\n",
      "Epoch 174/200\n",
      "469/469 [==============================] - 0s 922us/step - loss: 0.1892 - accuracy: 0.9467 - val_loss: 0.1912 - val_accuracy: 0.9456\n",
      "Epoch 175/200\n",
      "469/469 [==============================] - 0s 919us/step - loss: 0.1887 - accuracy: 0.9468 - val_loss: 0.1907 - val_accuracy: 0.9459\n",
      "Epoch 176/200\n",
      "469/469 [==============================] - 0s 950us/step - loss: 0.1882 - accuracy: 0.9469 - val_loss: 0.1903 - val_accuracy: 0.9453\n",
      "Epoch 177/200\n",
      "469/469 [==============================] - 0s 932us/step - loss: 0.1877 - accuracy: 0.9470 - val_loss: 0.1898 - val_accuracy: 0.9459\n",
      "Epoch 178/200\n",
      "469/469 [==============================] - 0s 863us/step - loss: 0.1872 - accuracy: 0.9471 - val_loss: 0.1895 - val_accuracy: 0.9462\n",
      "Epoch 179/200\n",
      "469/469 [==============================] - 0s 816us/step - loss: 0.1867 - accuracy: 0.9475 - val_loss: 0.1889 - val_accuracy: 0.9464\n",
      "Epoch 180/200\n",
      "469/469 [==============================] - 0s 899us/step - loss: 0.1862 - accuracy: 0.9475 - val_loss: 0.1885 - val_accuracy: 0.9460\n",
      "Epoch 181/200\n",
      "469/469 [==============================] - 0s 773us/step - loss: 0.1857 - accuracy: 0.9478 - val_loss: 0.1881 - val_accuracy: 0.9464\n",
      "Epoch 182/200\n",
      "469/469 [==============================] - 0s 973us/step - loss: 0.1852 - accuracy: 0.9477 - val_loss: 0.1877 - val_accuracy: 0.9464\n",
      "Epoch 183/200\n",
      "469/469 [==============================] - 0s 924us/step - loss: 0.1848 - accuracy: 0.9480 - val_loss: 0.1874 - val_accuracy: 0.9464\n",
      "Epoch 184/200\n",
      "469/469 [==============================] - 0s 849us/step - loss: 0.1843 - accuracy: 0.9480 - val_loss: 0.1868 - val_accuracy: 0.9466\n",
      "Epoch 185/200\n",
      "469/469 [==============================] - 0s 832us/step - loss: 0.1838 - accuracy: 0.9480 - val_loss: 0.1863 - val_accuracy: 0.9471\n",
      "Epoch 186/200\n",
      "469/469 [==============================] - 0s 852us/step - loss: 0.1833 - accuracy: 0.9485 - val_loss: 0.1859 - val_accuracy: 0.9470\n",
      "Epoch 187/200\n",
      "469/469 [==============================] - 0s 790us/step - loss: 0.1828 - accuracy: 0.9482 - val_loss: 0.1855 - val_accuracy: 0.9470\n",
      "Epoch 188/200\n",
      "469/469 [==============================] - 0s 777us/step - loss: 0.1824 - accuracy: 0.9485 - val_loss: 0.1851 - val_accuracy: 0.9472\n",
      "Epoch 189/200\n",
      "469/469 [==============================] - 0s 849us/step - loss: 0.1819 - accuracy: 0.9487 - val_loss: 0.1847 - val_accuracy: 0.9470\n",
      "Epoch 190/200\n",
      "469/469 [==============================] - 0s 772us/step - loss: 0.1814 - accuracy: 0.9489 - val_loss: 0.1843 - val_accuracy: 0.9472\n",
      "Epoch 191/200\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9491 - val_loss: 0.1838 - val_accuracy: 0.9470\n",
      "Epoch 192/200\n",
      "469/469 [==============================] - 0s 830us/step - loss: 0.1805 - accuracy: 0.9490 - val_loss: 0.1834 - val_accuracy: 0.9477\n",
      "Epoch 193/200\n",
      "469/469 [==============================] - 0s 835us/step - loss: 0.1801 - accuracy: 0.9492 - val_loss: 0.1830 - val_accuracy: 0.9473\n",
      "Epoch 194/200\n",
      "469/469 [==============================] - 0s 783us/step - loss: 0.1796 - accuracy: 0.9494 - val_loss: 0.1826 - val_accuracy: 0.9472\n",
      "Epoch 195/200\n",
      "469/469 [==============================] - 0s 830us/step - loss: 0.1792 - accuracy: 0.9497 - val_loss: 0.1822 - val_accuracy: 0.9476\n",
      "Epoch 196/200\n",
      "469/469 [==============================] - 0s 765us/step - loss: 0.1787 - accuracy: 0.9497 - val_loss: 0.1818 - val_accuracy: 0.9479\n",
      "Epoch 197/200\n",
      "469/469 [==============================] - 0s 773us/step - loss: 0.1783 - accuracy: 0.9497 - val_loss: 0.1813 - val_accuracy: 0.9476\n",
      "Epoch 198/200\n",
      "469/469 [==============================] - 0s 911us/step - loss: 0.1778 - accuracy: 0.9498 - val_loss: 0.1810 - val_accuracy: 0.9479\n",
      "Epoch 199/200\n",
      "469/469 [==============================] - 0s 851us/step - loss: 0.1773 - accuracy: 0.9503 - val_loss: 0.1807 - val_accuracy: 0.9478\n",
      "Epoch 200/200\n",
      "469/469 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9503 - val_loss: 0.1801 - val_accuracy: 0.9476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x177b76450>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=200,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 408us/step - loss: 0.1801 - accuracy: 0.9476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18014077842235565, 0.9476000070571899]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
